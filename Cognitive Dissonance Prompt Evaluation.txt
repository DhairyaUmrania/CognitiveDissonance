#!/usr/bin/env python3
"""
Simple manual tester for cognitive dissonance prompts
Clean, readable version with only essential features
"""

import json
import pandas as pd
from sklearn.metrics import accuracy_score, classification_report
import re

# =============================================================================
# PROMPT TEMPLATES
# =============================================================================

def get_prompts():
    """Define prompt templates"""
    return {
        "main": """You are an annotator for an NLP task that tries to detect cognitive dissonance from social media. Your job is to annotate consonance and dissonance from Twitter posts. The task conceptualizes consonance and dissonance as a form of discourse relation between two phrases or discourse units.

To annotate, you are given:
1. A Twitter post for context
2. Two discourse units (phrases) extracted from that post
3. You must determine the relationship between these two discourse units

Follow these steps in order:

Step 1: Is the parsing and segmentation adequate to judge the relationship?
- If NO â†’ Label is NEITHER

Step 2: Are the two beliefs logically contradictory (directly or indirectly)?
- If YES â†’ Label is DISSONANCE

Step 3: Are the two beliefs in agreement (supporting, repeating, clarifying, agreeing)?
- If YES â†’ Label is CONSONANCE

If none of the above apply â†’ Label is NEITHER

**Post:** "{message}"
**Discourse Unit 1:** "{du1}"
**Discourse Unit 2:** "{du2}"

Answer each step with YES or NO, then provide your final label.

Your output should look like the following dictionary, without any extra text or details:
{{"steps": ["YES", "NO", "YES"], "label": "CONSONANCE"}}""",

        "cot": """You are an annotator for an NLP task that detects cognitive dissonance from social media. Your job is to annotate the relationship between two discourse units from Twitter posts.

Given:
- A Twitter post (context)
- Two discourse units (phrases) from that post

Step 1: Is the parsing and segmentation adequate to judge the relationship?
If NO â†’ Label is NEITHER

Step 2: Are the two beliefs logically contradictory (directly or indirectly)?
If YES â†’ Label is DISSONANCE

Step 3: Are the two beliefs in agreement (supporting, repeating, clarifying, agreeing)?
If YES â†’ Label is CONSONANCE

If none apply â†’ Label is NEITHER

**Post:** "{message}"
**Discourse Unit 1:** "{du1}"
**Discourse Unit 2:** "{du2}"

Think through each step carefully, then output:
{{"steps": ["answer1", "answer2", "answer3"], "label": "FINAL_LABEL"}}""",

        "simple": """You are an annotator detecting cognitive dissonance from social media posts.

**Task:** Determine the relationship between two discourse units.

**Post:** "{message}"
**Unit 1:** "{du1}"
**Unit 2:** "{du2}"

**Rules:**
- DISSONANCE: Units contradict each other
- CONSONANCE: Units support/agree with each other  
- NEITHER: Units are unrelated or unclear

Output format:
{{"label": "YOUR_ANSWER"}}"""
    }

# =============================================================================
# DATA LOADING
# =============================================================================

def load_data():
    """Load the dissonance dataset"""
    print("Loading data...")
    
    # Try to find dataset files
    files = ['data/train_big.json', 'train_big.json', 'data/dev.json', 'dev.json']
    
    for filepath in files:
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            print(f"Loaded {len(data)} examples from {filepath}")
            return data
        except FileNotFoundError:
            continue
    
    print("No dataset files found!")
    print("Make sure you have the dissonance dataset in your directory")
    return None

def create_test_sample(data, n=10):
    """Create a balanced test sample"""
    if not data:
        return None
    
    # Group by label
    groups = {'D': [], 'C': [], 'N': []}
    for example in data:
        label = example['label']
        if label in groups:
            groups[label].append(example)
    
    # Take samples from each group
    sample = []
    per_group = n // 3
    
    for label, examples in groups.items():
        sample.extend(examples[:per_group])
    
    print(f"Created test sample with {len(sample)} examples")
    return sample

# =============================================================================
# RESPONSE PARSING
# =============================================================================

def parse_response(response):
    """Extract classification from JSON-formatted LLM response"""
    response = response.strip()
    
    # Try to parse JSON response
    try:
        import json as json_lib
        # Handle cases where response might have extra text
        if '{' in response and '}' in response:
            start = response.find('{')
            end = response.rfind('}') + 1
            json_str = response[start:end]
            parsed = json_lib.loads(json_str)
            
            # Extract label
            label = parsed.get('label', '').upper()
            if label in ['DISSONANCE', 'CONSONANCE', 'NEITHER']:
                # Convert to single letter for consistency
                label_map = {'DISSONANCE': 'D', 'CONSONANCE': 'C', 'NEITHER': 'N'}
                return label_map[label], parsed.get('steps', [])
    except:
        pass
    
    # Fallback parsing if JSON fails
    response_lower = response.lower()
    if 'dissonance' in response_lower:
        return 'D', []
    elif 'consonance' in response_lower:
        return 'C', []
    elif 'neither' in response_lower:
        return 'N', []
    
    return 'N', []  # Default to Neither

# =============================================================================
# TESTING FUNCTIONS
# =============================================================================

def test_prompt(prompt_name, test_data):
    """Test a prompt on the data"""
    prompts = get_prompts()
    template = prompts[prompt_name]
    
    print(f"\nTesting: {prompt_name}")
    print("=" * 50)
    
    predictions = []
    true_labels = []
    step_responses = []
    
    for i, example in enumerate(test_data):
        print(f"\n--- Example {i+1}/{len(test_data)} ---")
        print(f"True answer: {example['label']}")
        print(f"Message: {example['message'][:80]}...")
        print(f"Unit 1: {example['du1']}")
        print(f"Unit 2: {example['du2']}")
        
        # Format prompt
        prompt = template.format(
            message=example['message'],
            du1=example['du1'],
            du2=example['du2']
        )
        
        print(f"\nCOPY THIS TO YOUR LLM:")
        print("-" * 30)
        print(prompt)
        print("-" * 30)
        
        # Get response
        response = input("\nPaste LLM response: ").strip()
        if not response:
            print("Skipping empty response")
            continue
        
        # Parse response
        pred, steps = parse_response(response)
        correct = "CORRECT" if pred == example['label'] else "INCORRECT"
        
        print(f"Result: {pred} ({correct})")
        if steps:
            print(f"Steps: {steps}")
        
        # Store results
        predictions.append(pred)
        true_labels.append(example['label'])
        step_responses.append(steps)
        
        # Continue?
        if i < len(test_data) - 1:
            cont = input("Continue? (Enter/n): ").strip()
            if cont.lower() == 'n':
                break
    
    return predictions, true_labels, step_responses

def show_results(predictions, true_labels, step_responses, prompt_name):
    """Display test results and error analysis"""
    if not predictions:
        print("No results to show!")
        return
    
    print(f"\nRESULTS: {prompt_name}")
    print("=" * 50)
    
    # Overall accuracy
    accuracy = accuracy_score(true_labels, predictions)
    print(f"Accuracy: {accuracy:.1%}")
    
    # Individual results with error analysis
    label_names = {'D': 'Dissonance', 'C': 'Consonance', 'N': 'Neither'}
    print(f"\nIndividual Results:")
    errors = []
    
    for i, (true, pred, steps) in enumerate(zip(true_labels, predictions, step_responses)):
        status = "CORRECT" if true == pred else "INCORRECT"
        print(f"  {i+1}: {label_names[true]} -> {label_names[pred]} ({status})")
        if steps:
            print(f"      Steps: {steps}")
        
        # Track errors for analysis
        if true != pred:
            errors.append({
                'example': i+1,
                'true': true,
                'predicted': pred,
                'steps': steps
            })
    
    # Error analysis
    if errors:
        print(f"\nERROR ANALYSIS:")
        print(f"Found {len(errors)} errors. Common patterns:")
        
        # Count error types
        error_types = {}
        for error in errors:
            error_type = f"{error['true']} -> {error['predicted']}"
            error_types[error_type] = error_types.get(error_type, 0) + 1
        
        for error_type, count in error_types.items():
            print(f"  {error_type}: {count} cases")
        
        print(f"\nSUGGESTIONS:")
        if 'D -> C' in error_types or 'D -> N' in error_types:
            print("  - Model struggling to detect contradictions")
            print("  - Consider adding clearer contradiction examples")
        if 'C -> N' in error_types:
            print("  - Model not recognizing support relationships")
            print("  - Add more obvious agreement examples")
        if 'N -> D' in error_types or 'N -> C' in error_types:
            print("  - Model over-interpreting neutral cases")
            print("  - Emphasize 'unrelated' criteria in prompt")
    
    # Detailed breakdown
    print(f"\nDetailed Classification Report:")
    try:
        report = classification_report(
            true_labels, predictions, 
            target_names=['Consonance', 'Dissonance', 'Neither'],
            labels=['C', 'D', 'N'],
            zero_division=0
        )
        print(report)
    except:
        print("Could not generate detailed report")
    
    return accuracy

# =============================================================================
# MAIN FUNCTION
# =============================================================================

def main():
    """Main testing workflow"""
    print("ðŸš€ COGNITIVE DISSONANCE PROMPT TESTER")
    print("=" * 50)
    
    # Load data
    data = load_data()
    if not data:
        return
    
    # Create test sample
    n_examples = input("How many examples to test? (5-20 recommended): ").strip()
    try:
        n_examples = int(n_examples)
    except:
        n_examples = 10
    
    test_data = create_test_sample(data, n_examples)
    if not test_data:
        return
    
    # Choose prompt
    prompts = get_prompts()
    print(f"\nAvailable prompts:")
    for i, name in enumerate(prompts.keys(), 1):
        print(f"  {i}. {name}")
    
    choice = input(f"Choose prompt (1-{len(prompts)}): ").strip()
    try:
        prompt_name = list(prompts.keys())[int(choice) - 1]
    except:
        prompt_name = 'simple'
    
    # Run test
    print(f"\nðŸŽ¯ Testing '{prompt_name}' on {len(test_data)} examples")
    print("Goal: Achieve 80% accuracy")
    predictions, true_labels, step_responses = test_prompt(prompt_name, test_data)
    
    # Show results with error analysis
    accuracy = show_results(predictions, true_labels, step_responses, prompt_name)
    
    # Check if we hit target
    if accuracy >= 0.8:
        print(f"\nðŸŽ‰ SUCCESS! Achieved {accuracy:.1%} accuracy (target: 80%)")
    else:
        print(f"\nðŸ“Š Current: {accuracy:.1%} accuracy (target: 80%)")
        print("ðŸ’¡ Use error analysis above to improve the prompt")
    
    # Test another?
    again = input(f"\nTest another prompt? (y/n): ").strip().lower()
    if again == 'y':
        main()

if __name__ == "__main__":
    main()